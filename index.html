<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Inverse Reinforcement Learning with Multiple Morphologies</title>
    <style>
        body { font-family: Arial, sans-serif; }
        .hidden { display: none; }

        /* General body styles */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #f4f4f9;
            color: #333;
            line-height: 1.6;
            margin: 0;
            padding: 0;
        }

        /* center h1 and button */
        h1 {
            text-align: center;
        }
      
        /* Header and titles */
        h1, h2, h3 {
            color: #333;
        }
      
        /* Container for content */
        .content {
            margin: 20px;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
      
        /* Style for navigation and language switch button */
        button {
            background-color: #5c67f2;
            color: #fff;
            border: none;
            padding: 10px 20px;
            margin: 20px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
        }
      
        button:hover {
            background-color: #4a54e1;
        }
      
        /* Link styles */
        a {
            color: #5c67f2;
            text-decoration: none;
        }
      
        a:hover {
            text-decoration: underline;
        }
      
        /* List styles */
        ul {
            padding-left: 20px;
        }
      
        li {
            margin-bottom: 10px;
        }
      
        /* Media queries for responsiveness */
        @media (max-width: 768px) {
            .content {
                margin: 10px;
                padding: 10px;
            }
          
            button {
                padding: 8px 16px;
            }
        }
    </style>
</head>
<body>
    <h1>Inverse Reinforcement Learning with Multiple Morphologies</h1>
    <button onclick="toggleLanguage()">Switch Language</button>    
    <div id="english" class="content">
        <a href="presentation.pptx">Presentation Slides</a>
        <a href="repport.pptx">Final Report</a>

        <h2>Project Description</h2>
        <p>In the domain of reinforcement learning (RL), an agent learns optimal decision-making by interacting with an environment, guided by a specific reward signal. However, explicitly defining or even determining a reward function in real-world applications can be challenging. Inverse reinforcement learning (IRL) seeks to infer this reward function based on observed behavior.</p>
        <p>This project aims to derive a universal reward function that can effectively accommodate agents with various morphologies—a significant challenge given the complex nature of designing "properly shaped" reward functions. We are exploring the use of a transformer architecture to formulate this reward function. Our approach involves training a policy for one morphology using data from another in-domain morphology with distinct dynamics, thus addressing major challenges in reward and policy generalization across different agent forms.</p>
        <p>There is a strong motivation in enhancing safety in RL. With this method, we may be able to transfer the constraints or rules governing a set of agents to a different kind of agent.</p>
        
        <h2>Reports</h2>
        <p>Some of the dates may not be entirely accurate, since I estimated them from my notes and memory.</p>
        
        <!-- Detailed reports here -->
        <!-- Continue to include your specific report entries as structured below -->
        
        <h3>4/29/2023</h3>
        <p>Task: Prepare presentation and finalize report.</p>
        <p>Implemented a heterogeneous transformer architectures</p>
        
        <h3>4/21/2024</h3>
        <p>Task: Continue executing final experiments.</p>

        <h3>4/15/2024</h3>
        <p>Task: Ongoing experiments with adjustments to architecture for enhanced performance and stability.</p>
        <p>Challenges:</p>
        <ul>
            <li>ML-IRL with the transformer-based reward, when trained on a single morphology, performs approximately 30% worse than the MLP-based reward.</li>
            <li>Stability issues persist with the transformer reward, despite various experimental adjustments.</li>
            <li>When applied to multiple morphologies, the transformer reward underperforms compared to expert benchmarks by roughly 38%.</li>
            <li>Reward transfer using the transformer model fails to converge, even for familiar morphologies.</li>
        </ul>
        <p>Achievements:</p>
        <ul>
            <li>Resolved issues related to transformer model size, significantly boosting performance.</li>
            <li>Stability improvements noted when the transformer reward is applied across multiple morphologies.</li>
            <li>Both full transformer architecture and transformer encoder-only setups show promising results, the latter being beneficial for state-only ML-IRL and potential transfer learning applications.</li>
        </ul>

        <h3>4/8/2024</h3>
        <p>Task: Implemented initial transformer architecture inspired by AnyMorph and started the primary experiments.</p>
        <p>Implemented the initial transformer architecture based off the AnyMorph architecture and started running the main experiments.</p>
        <p>Observations:</p>
        <ul>
            <li>MLP reward models closely match expert returns, yet exhibit fluctuations and sporadic failures.</li>
            <li>The ML-IRL feedback loop was particularly sensitive to the transformer model size, requiring considerable adjustments.</li>
        </ul>

        <h3>4/1/2024</h3>
        <p>With assistance from Adriana, one of Glen's students, resolved issues with the Tianshou library affecting reward modifications.</p>

        <h3>3/18/2024</h3>
        <p>Encountered a major setback as the ML-IRL algorithm seemed to ignore the modified reward function and relied on the original rewards.</p>

        <h3>3/11/2024</h3>
        <p>Implemented and trained Soft Actor-Critic (SAC) policies for various morphologies and began integrating the ML-IRL algorithm into our codebase.</p>

        <h3>3/4/2024</h3>
        <p>Integrated the ModularRL environments into our codebase, adapting them for the latest version of Gymnasium.</p>

        <h3>2/26/2024</h3>
        <p>Reviewed the ML-IRL paper and set up a work environment in Glen’s lab for remote model training. Opted to develop our ML-IRL implementation from scratch instead of using an unmaintained codebase.</p>
        <a href="https://arxiv.org/abs/2210.01282">ML-IRL paper</a>

        <h3>2/5/2024</h3>
        <p>Reviewed literature of single-policy-multiple-morphology including the AnyMorph paper.</p>
        <ul>
          <li><a href="https://arxiv.org/abs/2010.01856">AnyMorph</a></li>
          <li><a href="https://arxiv.org/abs/2007.04976">Shared Modular Policies</a></li>
          <li><a href="https://arxiv.org/abs/2007.04976">Amorpheus</a></li>
        </ul>

        <h3>Start of semester</h3>
        <p>Dedicated time to identifying a suitable project, I delved into various concepts of reinforcement learning (RL) and deep reinforcement learning (Deep RL).
           Additionally, I explored popular frameworks and libraries, including Gymnasium.</p>

        <p>
          Ultimately, we decided to pursue a project that integrates Maximum-likelihood Inverse Reinforcement Learning (ML-IRL) 
          with a morphology-agnostic transformer architecture, known as AnyMorph.</p>
    </div>
    
    <div id="french" class="content hidden">

    </div>

    <script>
        function toggleLanguage() {
            var english = document.getElementById('english');
            var french = document.getElementById('french');
            if (english.classList.contains('hidden')) {
                english.classList.remove('hidden');
                french.classList.add('hidden');
            } else {
                english.classList.add('hidden');
                french.classList.remove('hidden');
            }
        }
    </script>
</body>
</html>